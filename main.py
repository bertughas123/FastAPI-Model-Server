"""
FastAPI Model Server - Ana Uygulama
AÅŸama 3: Gemini API ile AkÄ±llÄ± Analiz
"""
from fastapi import FastAPI, HTTPException, status, Request
from fastapi.responses import JSONResponse
from typing import Dict, Any
from collections import deque, defaultdict
import time
from datetime import datetime
from models.dummy_model import ml_model

# Schemas import
from schemas.requests import PredictRequest, MetricsQueryRequest
from schemas.responses import PredictResponse, HealthResponse
from schemas.metrics import AggregatedMetrics, MetricThresholds, GeminiAnalysisReport

# Services import
from services.metrics_tracker import metrics_tracker
from services.gemini_analyzer import gemini_analyzer

# FastAPI uygulamasÄ± oluÅŸtur
app = FastAPI(
    title="FastAPI Model Server",
    description="ML Model Serving ve Performans Ä°zleme API'si (AÅŸama 3: Gemini AI)",
    version="3.0.0",
    docs_url="/docs",  # Swagger UI: http://localhost:8000/docs
    redoc_url="/redoc"  # ReDoc: http://localhost:8000/redoc
)





# ============================================================================
# GLOBAL DEÄÄ°ÅKENLER
# ============================================================================

# Uygulama baÅŸlangÄ±Ã§ zamanÄ± (uptime hesabÄ± iÃ§in)
app_start_time = time.time()


# ============================================================================
# RATE LIMITING SÄ°STEMÄ°
# ============================================================================

class RateLimiter:
    """
    IP tabanlÄ± basit rate limiter
    """
    
    def __init__(self, max_requests: int = 10, time_window: int = 60):
        """
        Args:
            max_requests: Zaman penceresi iÃ§inde maksimum istek sayÄ±sÄ±
            time_window: Zaman penceresi (saniye)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        
        # Her IP iÃ§in timestamp listesi
        # defaultdict: Yeni IP geldiÄŸinde otomatik boÅŸ deque oluÅŸturur
        self.requests: Dict[str, deque] = defaultdict(deque)
    
    def is_allowed(self, client_ip: str) -> bool:
        """
        Ä°steÄŸin izin verilip verilmeyeceÄŸini kontrol et
        
        Args:
            client_ip: Ä°stemci IP adresi
            
        Returns:
            True: Ä°stek kabul edilebilir
            False: Rate limit aÅŸÄ±ldÄ±
        """
        current_time = time.time()
        request_times = self.requests[client_ip]
        
        # Eski timestamp'leri temizle (time_window dÄ±ÅŸÄ±ndakiler)
        while request_times and request_times[0] < current_time - self.time_window:
            request_times.popleft()
        
        # Limit kontrolÃ¼
        if len(request_times) >= self.max_requests:
            return False  # Limit aÅŸÄ±ldÄ±
        
        # Yeni timestamp ekle
        request_times.append(current_time)
        return True


# Global rate limiter instance
# Dakikada maksimum 10 istek (60 saniyede 10)
rate_limiter = RateLimiter(max_requests=10, time_window=60)


# ============================================================================
# LIFECYCLE EVENTS (Uygulama YaÅŸam DÃ¶ngÃ¼sÃ¼)
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """
    Uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸÄ±r
    Burada model yÃ¼kleme, veritabanÄ± baÄŸlantÄ±sÄ± gibi iÅŸlemler yapÄ±lÄ±r
    """
    print("=" * 50)
    print("ğŸš€ FastAPI Model Server baÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    # ML modelini yÃ¼kle
    ml_model.load_model()
    
    print("=" * 50)
    print("âœ… Sunucu hazÄ±r!")
    print("ğŸ“– DokÃ¼mantasyon: http://localhost:8000/docs")
    print("=" * 50)


@app.on_event("shutdown")
async def shutdown_event():
    """Uygulama kapatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸÄ±r"""
    print("ğŸ”´ Sunucu kapatÄ±lÄ±yor...")


# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get(
    "/",
    summary="Ana Sayfa",
    description="API'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrular"
)
async def root():
    """Ana endpoint - API'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir"""
    return {
        "message": "FastAPI Model Server Ã§alÄ±ÅŸÄ±yor! ğŸš€",
        "documentation": "/docs",
        "health_check": "/health"
    }


@app.get(
    "/health",
    response_model=HealthResponse,
    summary="SaÄŸlÄ±k KontrolÃ¼",
    description="Servis ve model durumunu kontrol eder",
    status_code=status.HTTP_200_OK
)
async def health_check():
    """
    SaÄŸlÄ±k kontrolÃ¼ endpoint'i
    
    Returns:
        HealthResponse: Servis durumu bilgileri
    
    Ã–NEMLÄ°: async def kullanÄ±yoruz Ã§Ã¼nkÃ¼ FastAPI bu sayede:
    - Birden fazla /health isteÄŸini aynÄ± anda iÅŸleyebilir
    - Sistem kaynaklarÄ±nÄ± daha verimli kullanÄ±r
    - Daha yÃ¼ksek throughput saÄŸlar
    """
    uptime = time.time() - app_start_time
    
    return HealthResponse(
        status="healthy" if ml_model.is_loaded else "unhealthy",
        model_loaded=ml_model.is_loaded,
        model_name=ml_model.model_name,
        model_version=ml_model.version,
        timestamp=datetime.utcnow().isoformat() + "Z",
        uptime_seconds=round(uptime, 2)
    )


@app.post(
    "/predict",
    response_model=PredictResponse,
    summary="Tahmin Yap (Rate Limited)",
    description="Gelen metni analiz eder ve sentiment tahmini yapar. Dakikada maksimum 10 istek.",
    status_code=status.HTTP_200_OK
)
async def predict(request: PredictRequest, http_request: Request):
    """
    ML model tahmini endpoint'i (Rate Limited)
    
    Args:
        request: PredictRequest ÅŸemasÄ±na uygun istek body'si
        http_request: FastAPI Request objesi (IP adresi iÃ§in)
        
    Returns:
        PredictResponse: Tahmin sonuÃ§larÄ±
        
    Raises:
        HTTPException: 
            - 429: Rate limit aÅŸÄ±ldÄ± (dakikada 10 istekten fazla)
            - 503: Model yÃ¼klÃ¼ deÄŸil
            - 500: Tahmin hatasÄ±
    """
    
    # Rate limit kontrolÃ¼
    client_ip = http_request.client.host
    
    if not rate_limiter.is_allowed(client_ip):
        # Limit aÅŸÄ±ldÄ± - 429 hatasÄ± fÄ±rlat
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"Rate limit aÅŸÄ±ldÄ±. Dakikada maksimum {rate_limiter.max_requests} istek yapabilirsiniz."
        )
    
    # Model durumu kontrolÃ¼
    if not ml_model.is_loaded:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Model henÃ¼z yÃ¼klenmedi. LÃ¼tfen daha sonra tekrar deneyin."
        )
    
    try:
        # Model tahmini yap
        # NOT: GerÃ§ek async model iÃ§in: await model.predict_async(request.text)
        prediction = ml_model.predict(request.text)
        
        # Metrik kaydet
        metric = metrics_tracker.add_metric(
            sentiment=prediction["sentiment"],
            confidence=prediction["confidence"],
            inference_time_ms=prediction["inference_time_ms"],
            input_length=len(request.text),
            model_version=ml_model.version
        )
        
        # YanÄ±tÄ± oluÅŸtur
        response = PredictResponse(
            sentiment=prediction["sentiment"],
            confidence=prediction["confidence"],
            inference_time_ms=prediction["inference_time_ms"],
            timestamp=datetime.utcnow().isoformat() + "Z",
            model_version=ml_model.version,
            metric=metric if request.include_metrics else None
        )
        
        return response
        
    except Exception as e:
        # Hata durumunda 500 Internal Server Error
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Tahmin sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        )


# ============================================================================
# METRÄ°K ENDPOÄ°NTLERÄ° (AÅŸama 2)
# ============================================================================

@app.post(
    "/metrics/aggregated",
    response_model=AggregatedMetrics,
    tags=["Metrics"],
    summary="Toplam Metrikleri Getir"
)
async def get_aggregated_metrics(query: MetricsQueryRequest):
    """
    Belirli zaman aralÄ±ÄŸÄ±ndaki toplam metrikleri dÃ¶ndÃ¼r
    
    Args:
        query: Zaman penceresi (dakika cinsinden)
        
    Returns:
        Toplanan metrikler (ortalamalar, daÄŸÄ±lÄ±mlar, vb.)
    """
    return metrics_tracker.get_aggregated_metrics(
        time_window_minutes=query.time_window_minutes
    )


@app.put(
    "/metrics/thresholds",
    response_model=MetricThresholds,
    tags=["Metrics"],
    summary="EÅŸik DeÄŸerlerini GÃ¼ncelle"
)
async def update_thresholds(thresholds: MetricThresholds):
    """
    Metrik eÅŸik deÄŸerlerini gÃ¼ncelle
    
    Args:
        thresholds: Yeni eÅŸik deÄŸerleri
        
    Returns:
        GÃ¼ncellenen eÅŸik deÄŸerleri
    """
    metrics_tracker.update_thresholds(thresholds)
    return thresholds


@app.get(
    "/metrics/count",
    tags=["Metrics"],
    summary="Toplam Metrik SayÄ±sÄ±"
)
async def get_metrics_count():
    """Toplam kaydedilmiÅŸ metrik sayÄ±sÄ±nÄ± dÃ¶ndÃ¼r"""
    return {
        "total_metrics": len(metrics_tracker.metrics),
        "description": "Uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±ndan beri kaydedilen toplam tahmin sayÄ±sÄ±"
    }


# ============================================================================
# GEMÄ°NÄ° AI ANALÄ°Z ENDPOÄ°NTÄ° (AÅŸama 3)
# ============================================================================

@app.post(
    "/analyze/performance",
    response_model=GeminiAnalysisReport,
    tags=["AI Analysis"],
    summary="Gemini ile Performans Analizi"
)
async def analyze_performance(query: MetricsQueryRequest):
    """
    Gemini AI kullanarak performans metriklerini analiz et
    
    Ä°ki zaman penceresi karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r:
    - GÃ¼ncel: Son X dakika
    - Ã–nceki: X*2 ile X dakika arasÄ±
    
    Args:
        query: Zaman penceresi (dakika cinsinden)
        
    Returns:
        Gemini'nin oluÅŸturduÄŸu analiz raporu
        
    Raises:
        HTTPException: Analiz hatasÄ± durumunda
    """
    # GÃ¼ncel metrikler
    current_metrics = metrics_tracker.get_aggregated_metrics(
        time_window_minutes=query.time_window_minutes
    )
    
    # Ã–nceki dÃ¶nem metrikleri (karÅŸÄ±laÅŸtÄ±rma iÃ§in)
    # Ã–rn: Son 60dk vs Ã¶nceki 60dk
    previous_metrics = metrics_tracker.get_aggregated_metrics(
        time_window_minutes=query.time_window_minutes * 2
    )
    
    # Gemini ile analiz et
    try:
        report = gemini_analyzer.analyze_performance(
            current_metrics=current_metrics,
            previous_metrics=previous_metrics
        )
        return report
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Analiz hatasÄ±: {str(e)}"
        )



# ============================================================================
# HATA YÃ–NETÄ°MÄ° (Error Handlers)
# ============================================================================

@app.exception_handler(404)
async def not_found_handler(request, exc):
    """Ã–zel 404 hata mesajÄ±"""
    return JSONResponse(
        status_code=404,
        content={
            "detail": "AradÄ±ÄŸÄ±nÄ±z endpoint bulunamadÄ±",
            "available_endpoints": ["/", "/health", "/predict"],
            "documentation": "/docs"
        }
    )
