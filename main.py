"""
FastAPI Model Server - Ana Uygulama
AÅŸama 1: Temel API Mimarisi
"""
from fastapi import FastAPI, HTTPException, status, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Dict, Any
from collections import deque, defaultdict
import time
from datetime import datetime
from models.dummy_model import ml_model

# FastAPI uygulamasÄ± oluÅŸtur
app = FastAPI(
    title="FastAPI Model Server",
    description="ML Model Serving ve Performans Ä°zleme API'si",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI: http://localhost:8000/docs
    redoc_url="/redoc"  # ReDoc: http://localhost:8000/redoc
)


# ============================================================================
# PYDANTIC MODELLER (Input/Output ÅemalarÄ±)
# ============================================================================

class PredictRequest(BaseModel):
    """Tahmin isteÄŸi iÃ§in veri modeli"""
    text: str = Field(
        ...,  # ... = zorunlu alan
        min_length=1,
        max_length=1000,
        description="Analiz edilecek metin",
        examples=["Bu Ã¼rÃ¼n gerÃ§ekten harika!"]
    )
    
    class Config:
        # Pydantic v2 iÃ§in JSON schema Ã¶rnekleri
        json_schema_extra = {
            "example": {
                "text": "Bu Ã¼rÃ¼n gerÃ§ekten harika!"
            }
        }


class PredictResponse(BaseModel):
    """Tahmin yanÄ±tÄ± iÃ§in veri modeli"""
    sentiment: str = Field(description="Tespit edilen duygu (positive/negative/neutral)")
    confidence: float = Field(description="Tahmin gÃ¼ven skoru (0-1 arasÄ±)")
    inference_time_ms: float = Field(description="Model Ã§Ä±karÄ±m sÃ¼resi (milisaniye)")
    timestamp: str = Field(description="Ä°stek zamanÄ± (ISO 8601)")
    model_version: str = Field(description="KullanÄ±lan model versiyonu")


class HealthResponse(BaseModel):
    """SaÄŸlÄ±k kontrolÃ¼ yanÄ±tÄ±"""
    status: str = Field(description="Servis durumu")
    model_loaded: bool = Field(description="Model yÃ¼klenme durumu")
    model_name: str = Field(description="Model adÄ±")
    model_version: str = Field(description="Model versiyonu")
    timestamp: str = Field(description="Kontrol zamanÄ±")
    uptime_seconds: float = Field(description="Servis Ã§alÄ±ÅŸma sÃ¼resi (saniye)")


# ============================================================================
# GLOBAL DEÄÄ°ÅKENLER
# ============================================================================

# Uygulama baÅŸlangÄ±Ã§ zamanÄ± (uptime hesabÄ± iÃ§in)
app_start_time = time.time()


# ============================================================================
# RATE LIMITING SÄ°STEMÄ°
# ============================================================================

class RateLimiter:
    """
    IP tabanlÄ± basit rate limiter
    
    NasÄ±l Ã‡alÄ±ÅŸÄ±r:
    1. Her IP iÃ§in son isteklerin timestamp'lerini deque'da tutar
    2. Yeni istek geldiÄŸinde eski timestamp'leri temizler (time_window dÄ±ÅŸÄ±ndakiler)
    3. Limit aÅŸÄ±lmÄ±ÅŸsa False dÃ¶ner, deÄŸilse yeni timestamp ekler ve True dÃ¶ner
    
    Neden deque?
    - deque (double-ended queue) baÅŸtan ve sondan O(1) ekleme/silme yapar
    - Liste kullanÄ±rsak pop(0) iÅŸlemi O(n) olur (yavaÅŸ)
    - Eski timestamp'leri soldan silmek Ã§ok hÄ±zlÄ±: popleft()
    """
    
    def __init__(self, max_requests: int = 10, time_window: int = 60):
        """
        Args:
            max_requests: Zaman penceresi iÃ§inde maksimum istek sayÄ±sÄ±
            time_window: Zaman penceresi (saniye)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        
        # Her IP iÃ§in timestamp listesi
        # defaultdict: Yeni IP geldiÄŸinde otomatik boÅŸ deque oluÅŸturur
        self.requests: Dict[str, deque] = defaultdict(deque)
    
    def is_allowed(self, client_ip: str) -> bool:
        """
        Ä°steÄŸin izin verilip verilmeyeceÄŸini kontrol et
        
        Args:
            client_ip: Ä°stemci IP adresi
            
        Returns:
            True: Ä°stek kabul edilebilir
            False: Rate limit aÅŸÄ±ldÄ±
        """
        current_time = time.time()
        request_times = self.requests[client_ip]
        
        # Eski timestamp'leri temizle (time_window dÄ±ÅŸÄ±ndakiler)
        while request_times and request_times[0] < current_time - self.time_window:
            request_times.popleft()
        
        # Limit kontrolÃ¼
        if len(request_times) >= self.max_requests:
            return False  # Limit aÅŸÄ±ldÄ±
        
        # Yeni timestamp ekle
        request_times.append(current_time)
        return True


# Global rate limiter instance
# Dakikada maksimum 10 istek (60 saniyede 10)
rate_limiter = RateLimiter(max_requests=10, time_window=60)


# ============================================================================
# LIFECYCLE EVENTS (Uygulama YaÅŸam DÃ¶ngÃ¼sÃ¼)
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """
    Uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸÄ±r
    Burada model yÃ¼kleme, veritabanÄ± baÄŸlantÄ±sÄ± gibi iÅŸlemler yapÄ±lÄ±r
    """
    print("=" * 50)
    print("ğŸš€ FastAPI Model Server baÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    # ML modelini yÃ¼kle
    ml_model.load_model()
    
    print("=" * 50)
    print("âœ… Sunucu hazÄ±r!")
    print("ğŸ“– DokÃ¼mantasyon: http://localhost:8000/docs")
    print("=" * 50)


@app.on_event("shutdown")
async def shutdown_event():
    """Uygulama kapatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸÄ±r"""
    print("ğŸ”´ Sunucu kapatÄ±lÄ±yor...")


# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get(
    "/",
    summary="Ana Sayfa",
    description="API'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrular"
)
async def root():
    """Ana endpoint - API'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir"""
    return {
        "message": "FastAPI Model Server Ã§alÄ±ÅŸÄ±yor! ğŸš€",
        "documentation": "/docs",
        "health_check": "/health"
    }


@app.get(
    "/health",
    response_model=HealthResponse,
    summary="SaÄŸlÄ±k KontrolÃ¼",
    description="Servis ve model durumunu kontrol eder",
    status_code=status.HTTP_200_OK
)
async def health_check():
    """
    SaÄŸlÄ±k kontrolÃ¼ endpoint'i
    
    Returns:
        HealthResponse: Servis durumu bilgileri
    
    Ã–NEMLÄ°: async def kullanÄ±yoruz Ã§Ã¼nkÃ¼ FastAPI bu sayede:
    - Birden fazla /health isteÄŸini aynÄ± anda iÅŸleyebilir
    - Sistem kaynaklarÄ±nÄ± daha verimli kullanÄ±r
    - Daha yÃ¼ksek throughput saÄŸlar
    """
    uptime = time.time() - app_start_time
    
    return HealthResponse(
        status="healthy" if ml_model.is_loaded else "unhealthy",
        model_loaded=ml_model.is_loaded,
        model_name=ml_model.model_name,
        model_version=ml_model.version,
        timestamp=datetime.utcnow().isoformat() + "Z",
        uptime_seconds=round(uptime, 2)
    )


@app.post(
    "/predict",
    response_model=PredictResponse,
    summary="Tahmin Yap (Rate Limited)",
    description="Gelen metni analiz eder ve sentiment tahmini yapar. Dakikada maksimum 10 istek.",
    status_code=status.HTTP_200_OK
)
async def predict(request: PredictRequest, http_request: Request):
    """
    ML model tahmini endpoint'i (Rate Limited)
    
    Args:
        request: PredictRequest ÅŸemasÄ±na uygun istek body'si
        http_request: FastAPI Request objesi (IP adresi iÃ§in)
        
    Returns:
        PredictResponse: Tahmin sonuÃ§larÄ±
        
    Raises:
        HTTPException: 
            - 429: Rate limit aÅŸÄ±ldÄ± (dakikada 10 istekten fazla)
            - 503: Model yÃ¼klÃ¼ deÄŸil
            - 500: Tahmin hatasÄ±
    """
    
    # Rate limit kontrolÃ¼
    client_ip = http_request.client.host
    
    if not rate_limiter.is_allowed(client_ip):
        # Limit aÅŸÄ±ldÄ± - 429 hatasÄ± fÄ±rlat
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"Rate limit aÅŸÄ±ldÄ±. Dakikada maksimum {rate_limiter.max_requests} istek yapabilirsiniz."
        )
    
    # Model durumu kontrolÃ¼
    if not ml_model.is_loaded:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Model henÃ¼z yÃ¼klenmedi. LÃ¼tfen daha sonra tekrar deneyin."
        )
    
    try:
        # Model tahmini yap
        # NOT: GerÃ§ek async model iÃ§in: await model.predict_async(request.text)
        prediction = ml_model.predict(request.text)
        
        # YanÄ±tÄ± oluÅŸtur ve dÃ¶ndÃ¼r
        return PredictResponse(
            sentiment=prediction["sentiment"],
            confidence=prediction["confidence"],
            inference_time_ms=prediction["inference_time_ms"],
            timestamp=datetime.utcnow().isoformat() + "Z",
            model_version=ml_model.version
        )
        
    except Exception as e:
        # Hata durumunda 500 Internal Server Error
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Tahmin sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        )


# ============================================================================
# HATA YÃ–NETÄ°MÄ° (Error Handlers)
# ============================================================================

@app.exception_handler(404)
async def not_found_handler(request, exc):
    """Ã–zel 404 hata mesajÄ±"""
    return JSONResponse(
        status_code=404,
        content={
            "detail": "AradÄ±ÄŸÄ±nÄ±z endpoint bulunamadÄ±",
            "available_endpoints": ["/", "/health", "/predict"],
            "documentation": "/docs"
        }
    )
